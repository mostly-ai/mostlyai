{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4be817d7",
   "metadata": {},
   "source": [
    "# MOSTLY AI vs. SDV Comparison - Referential Integrity Scenario  <a href=\"https://colab.research.google.com/github/mostly-ai/mostlyai/blob/main/docs/tutorials/sdv-comparison/referential-integrity-scenario/referential-integriry-scenario.ipynb\" target=\"_blank\"><img src=\"https://img.shields.io/badge/Open%20in-Colab-blue?logo=google-colab\" alt=\"Run on Colab\"></a>\n",
    "\n",
    "This notebook provides a comparison of the ability to preserve referential integrity between two leading synthetic data generation platforms:\n",
    "- **SDV (Synthetic Data Vault)** - Business Source License\n",
    "- **MOSTLY AI Synthetic Data SDK** - Apache 2.0 License - Open Source\n",
    "\n",
    "In this comparison, we are going to walk through the synthesis of a relational two-table structure using the [Berka dataset](https://github.com/mostly-ai/public-demo-data/tree/dev/berka/data).\n",
    "\n",
    "## Comparison Methodology\n",
    "\n",
    "1. **Data Preparation**: Load, inspect, and preprocess the multi-table dataset\n",
    "2. **Data Splitting**: Create train/test splits that maintain referential integrity\n",
    "3. **Model Training**: Train both SDV and MOSTLY AI generators on the training data\n",
    "4. **Synthetic Data Generation**: Generate synthetic datasets using both platforms\n",
    "5. **Performance Analysis**: Compare training time, generation speed, and data quality\n",
    "\n",
    "## Key Challenges in Multi-table Synthesis\n",
    "\n",
    "- **Referential Integrity**: Preserving temporal patterns in transaction data\n",
    "- **Data Quality**: Ensuring synthetic data maintains statistical properties and business logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4bb72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SDK in CLIENT mode\n",
    "!uv pip install -U mostlyai\n",
    "# Or install in LOCAL mode\n",
    "!uv pip install -U 'mostlyai[local]'  \n",
    "# Note: Restart kernel session after installation!\n",
    "\n",
    "!uv pip install -q scikit-learn seaborn lightgbm sdv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa97a2f5",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration\n",
    "\n",
    "First, let's load our dataset and examine its structure to understand:\n",
    "- Table schemas and data types\n",
    "- Relationships between tables\n",
    "- Business logic and constraints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01d5f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# GLIEF dataset hosted by MOSTLY AI\n",
    "base_url = \"https://raw.githubusercontent.com/mostly-ai/public-demo-data/dev/gleif/\"\n",
    "\n",
    "originals = {\n",
    "    \"organizations\": pd.read_csv(base_url + \"organizations.csv.gz\", compression=\"gzip\", low_memory=False),\n",
    "    \"relations\": pd.read_csv(base_url + \"relations.csv.gz\", compression=\"gzip\", low_memory=False),\n",
    "}\n",
    "\n",
    "# Print samples\n",
    "for k, df in originals.items():\n",
    "    print(\"===\", k, \"=== shape:\", df.shape)\n",
    "    display(df.sample(n=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d36a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataframes\n",
    "df_organizations = originals[\"organizations\"].copy()\n",
    "df_relations     = originals[\"relations\"].copy()\n",
    "\n",
    "# Print shapes\n",
    "print(\"\\nüìã Final dataframe shapes:\")\n",
    "print(f\"   - Organizations: {df_organizations.shape[0]:,} √ó {df_organizations.shape[1]}\")\n",
    "print(f\"   - Relations:     {df_relations.shape[0]:,} √ó {df_relations.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfed3a33",
   "metadata": {},
   "source": [
    "## 2. Strategic Data Splitting for Multi-Table Scenarios\n",
    "\n",
    "When dealing with multiple related tables, splitting data becomes more complex than simple random sampling. \n",
    "\n",
    "We need to consider:\n",
    "\n",
    "- **Referential Integrity**: Ensure foreign key relationships remain valid in both splits\n",
    "- **Business Logic**: Relations can only exist between organizations in the same split\n",
    "- **Data Leakage Prevention**: Avoid information bleeding between train/test sets\n",
    "\n",
    "**Our Approach:**\n",
    "1. Split organizations first (80/20 train/test)\n",
    "2. Assign relations based on participant customers\n",
    "3. Relations go to training set only if both `START_ID` and `END_ID` are in training set\n",
    "4. All other relations go to test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8512ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "ORG_KEY = \"ID\"\n",
    "REL_START = \"START_ID\"\n",
    "REL_END = \"END_ID\"\n",
    "\n",
    "print(\"Performing multi-table split (organizations / relations)...\")\n",
    "\n",
    "# Dtype alignment to avoid false negatives\n",
    "if df_relations[REL_START].dtype != df_organizations[ORG_KEY].dtype:\n",
    "    try:\n",
    "        df_relations[REL_START] = df_relations[REL_START].astype(df_organizations[ORG_KEY].dtype)\n",
    "        df_relations[REL_END]   = df_relations[REL_END].astype(df_organizations[ORG_KEY].dtype)\n",
    "    except Exception:\n",
    "        df_organizations[ORG_KEY] = df_organizations[ORG_KEY].astype(df_relations[REL_START].dtype)\n",
    "\n",
    "# Split organizations 80/20\n",
    "org_train, org_test = train_test_split(\n",
    "    df_organizations, test_size=0.2, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "print(\"Organizations split:\")\n",
    "print(f\"  - Training set: {len(org_train):,} orgs ({len(org_train)/len(df_organizations)*100:.1f}%)\")\n",
    "print(f\"  - Test set:     {len(org_test):,} orgs ({len(org_test)/len(df_organizations)*100:.1f}%)\")\n",
    "\n",
    "train_ids = set(org_train[ORG_KEY])\n",
    "test_ids  = set(org_test[ORG_KEY])\n",
    "\n",
    "# Sense checks\n",
    "overlap_ids = train_ids & test_ids\n",
    "if overlap_ids:\n",
    "    raise ValueError(f\"Split error: {len(overlap_ids)} overlapping {ORG_KEY} values between train/test.\")\n",
    "\n",
    "all_ids = train_ids | test_ids\n",
    "missing_from_split = set(df_organizations[ORG_KEY]) - all_ids\n",
    "if missing_from_split:\n",
    "    raise ValueError(f\"{len(missing_from_split)} {ORG_KEY} values not included in either split.\")\n",
    "\n",
    "rel = df_relations.copy()\n",
    "\n",
    "in_train = rel[REL_START].isin(train_ids) & rel[REL_END].isin(train_ids)\n",
    "in_test  = rel[REL_START].isin(test_ids)  & rel[REL_END].isin(test_ids)\n",
    "\n",
    "relations_train = rel[in_train].copy()\n",
    "relations_test  = rel[in_test].copy()\n",
    "\n",
    "print(\"Relations split (edges with both endpoints in the same partition):\")\n",
    "print(f\"  - Training relations: {len(relations_train):,} \"\n",
    "      f\"({len(relations_train)/len(df_relations)*100:.1f}%)\")\n",
    "print(f\"  - Test relations:     {len(relations_test):,} \"\n",
    "      f\"({len(relations_test)/len(df_relations)*100:.1f}%)\")\n",
    "\n",
    "unknown_nodes = set(pd.concat([rel[REL_START], rel[REL_END]]).unique()) - set(df_organizations[ORG_KEY])\n",
    "if unknown_nodes:\n",
    "    raise ValueError(f\"Found {len(unknown_nodes)} relation node IDs not present in organizations.\")\n",
    "\n",
    "org_train_out  = \"./data/organizations_train.parquet\"\n",
    "org_test_out   = \"./data/organizations_test.parquet\"\n",
    "rel_train_out  = \"./data/relations_train.parquet\"\n",
    "rel_test_out   = \"./data/relations_test.parquet\"\n",
    "\n",
    "org_train.to_parquet(org_train_out, index=False)\n",
    "org_test.to_parquet(org_test_out, index=False)\n",
    "relations_train.to_parquet(rel_train_out, index=False)\n",
    "relations_test.to_parquet(rel_test_out, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf2aaf1",
   "metadata": {},
   "source": [
    "## 3. SDV (Synthetic Data Vault) Implementation\n",
    "\n",
    "**About SDV:**\n",
    "- Business Source License Python library for synthetic data generation\n",
    "- Supports single-table and multi-table scenarios\n",
    "- Uses statistical modeling and machine learning approaches\n",
    "- Provides HMASynthesizer for hierarchical multi-table synthesis\n",
    "\n",
    "**Key Features:**\n",
    "- **Metadata Detection**: Automatically infers data types and relationships\n",
    "- **Relationship Modeling**: Handles parent-child table relationships\n",
    "- **Privacy Protection**: Generates synthetic data that preserves statistical properties while protecting individual privacy\n",
    "- **Extensible**: Multiple synthesizer options (GaussianCopula, CTGAN, CopulaGAN, etc.)\n",
    "\n",
    "**Limitations:**\n",
    "- Current version only supports one parent per child table\n",
    "- Complex multi-parent relationships require modeling simplification\n",
    "- Performance scales with data complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b481b0",
   "metadata": {},
   "source": [
    "### 3.1 SDV Metadata Configuration\n",
    "\n",
    "The relations table has TWO foreign keys (`START_ID` and `END_ID`) both referencing the organizations table. \n",
    "\n",
    "SDV v1.x only supports one parent per child, so we'll model one relationship explicitly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db08c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.multi_table import HMASynthesizer\n",
    "from sdv.metadata import Metadata\n",
    "\n",
    "metadata = Metadata.detect_from_dataframes(\n",
    "    data={\n",
    "        'organizations': org_train,\n",
    "        'relations': relations_train\n",
    "    },\n",
    "    infer_keys='primary_and_foreign'\n",
    ")\n",
    "\n",
    "metadata.add_relationship(\n",
    "    parent_table_name='organizations',\n",
    "    child_table_name='relations',\n",
    "    parent_primary_key='ID',\n",
    "    child_foreign_key='START_ID'\n",
    ")\n",
    "\n",
    "print(\"\\nüìã SDV Metadata Configuration:\")\n",
    "metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6104fc",
   "metadata": {},
   "source": [
    "### 3.2 SDV Model Training\n",
    "\n",
    "**HMASynthesizer Overview:**\n",
    "- **Hierarchical Modeling**: Learns parent-child relationships\n",
    "- **Statistical Approach**: Uses copulas and Gaussian distributions\n",
    "- **Multi-step Process**: \n",
    "  1. Preprocesses tables and infers constraints\n",
    "  2. Learns relationships between parent and child tables\n",
    "  3. Models individual table distributions\n",
    "  \n",
    "**Training Phases:**\n",
    "- **Preprocess Tables**: Data cleaning and type inference\n",
    "- **Learning Relationships**: Analyzing foreign key dependencies  \n",
    "- **Modeling Tables**: Learning statistical distributions for synthesis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef568118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize HMASynthesizer\n",
    "print(\"üîß Initializing HMASynthesizer...\")\n",
    "synthesizer = HMASynthesizer(metadata)\n",
    "\n",
    "# Training configuration\n",
    "synthesizer.fit({\n",
    "    'organizations': org_train,\n",
    "    'relations': relations_train\n",
    "})\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_minutes = (end_time - start_time) / 60\n",
    "\n",
    "print(f\"‚è±Ô∏è Total training time: {elapsed_minutes:.2f} minutes\")\n",
    "print(f\"üìà Training data: {len(org_train):,} organizations, {len(relations_train):,} relations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6781baea",
   "metadata": {},
   "source": [
    "### 3.3 SDV Synthetic Data Generation\n",
    "\n",
    "**Generation Process:**\n",
    "- **Scale Parameter**: Controls the number of synthetic records (1.0 = same size as training data)\n",
    "- **Hierarchical Generation**: First generates parent records (customers), then child records (transfers)\n",
    "- **Relationship Preservation**: Ensures all synthetic transfers reference valid synthetic customers\n",
    "- **Statistical Sampling**: Uses learned distributions to create realistic synthetic data\n",
    "\n",
    "We'll create just 10% of the total number of records to reduce generation time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fca156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "sdv_synthetic_data = synthesizer.sample(scale=0.25)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_minutes = (end_time - start_time) / 60\n",
    "\n",
    "# Performance metrics\n",
    "synthetic_orgs_count = len(sdv_synthetic_data['organizations'])\n",
    "synthetic_rels_count = len(sdv_synthetic_data['relations'])\n",
    "total_synthetic_records = synthetic_orgs_count + synthetic_rels_count\n",
    "generation_rate = total_synthetic_records / (end_time - start_time)\n",
    "\n",
    "print(f\"‚è±Ô∏è  Generation time: {elapsed_minutes:.2f} minutes\")\n",
    "print(f\"üöÄ Generation rate: {generation_rate:,.0f} records/second\")\n",
    "print(f\"üìä Generated {synthetic_orgs_count:,} synthetic organizations\")\n",
    "print(f\"üìä Generated {synthetic_rels_count:,} synthetic relations\")\n",
    "\n",
    "# Data quality verification\n",
    "relations_per_org = (\n",
    "    synthetic_rels_count / synthetic_orgs_count if synthetic_orgs_count > 0 else 0\n",
    ")\n",
    "\n",
    "print(f\"\\n Generation Quality Metrics:\")\n",
    "print(f\"   - Relations per organization: {relations_per_org:.1f}\")\n",
    "print(f\"   - Scale factor achieved: {synthetic_orgs_count / len(org_train):.2f}x\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd8fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save SDV synthetic data for comparison\n",
    "output_folder = './data/referential-integrity/'\n",
    "\n",
    "synthetic_files = {\n",
    "    'organizations': f'{output_folder}sdv_organizations.parquet',\n",
    "    'relations': f'{output_folder}sdv_relations.parquet',\n",
    "}\n",
    "\n",
    "for table_name, file_path in synthetic_files.items():\n",
    "    sdv_synthetic_data[table_name].to_parquet(file_path, index=False)\n",
    "    print(f\"üíæ Saved {table_name} synthetic data to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1078f37",
   "metadata": {},
   "source": [
    "## 4. MOSTLY AI Implementation\n",
    "\n",
    "**About MOSTLY AI Synthetic Data SDK:**\n",
    "- Open-source (Apache 2 license) synthetic data SDK with advanced AI capabilities\n",
    "- Also cloud-based service with enterprise-grade security and compliance\n",
    "- Supports complex multi-table scenarios with multiple foreign keys\n",
    "\n",
    "**Key Advantages:**\n",
    "- **Advanced AI Models**: Utilizes state-of-the-art generative AI including language models\n",
    "- **Multi-Parent Support**: Can handle complex relationships (multiple foreign keys per table)\n",
    "- **Mixed Data Types**: Excels at both tabular and text data synthesis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf36e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mostlyai.sdk import MostlyAI\n",
    "\n",
    "# Initialize MOSTLY AI Synthetic Data SDK\n",
    "mostly = MostlyAI(local=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dc0ac3",
   "metadata": {},
   "source": [
    "### 4.1 MOSTLY AI Configuration\n",
    "\n",
    "As before, the relations table has TWO foreign keys (`START_ID` and `END_ID`) both referencing the organizations table. \n",
    "\n",
    "MOSTLY AI supports multiple foreign keys referencing our subject table (organizations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da6c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'name': 'GLEIF Organizations & Relations Generator',\n",
    "    'tables': [\n",
    "        {\n",
    "            'name': 'organizations',\n",
    "            'data': org_train,\n",
    "            'primary_key': 'ID',  # first column in organizations\n",
    "            'tabular_model_configuration': {\n",
    "                'enable_model_report': False\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'relations',\n",
    "            'data': relations_train,\n",
    "            'foreign_keys': [\n",
    "                {\n",
    "                    'column': 'START_ID',\n",
    "                    'referenced_table': 'organizations',\n",
    "                    'referenced_column': 'ID',\n",
    "                    'is_context': True\n",
    "                },\n",
    "                {\n",
    "                    'column': 'END_ID',\n",
    "                    'referenced_table': 'organizations',\n",
    "                    'referenced_column': 'ID',\n",
    "                    'is_context': False\n",
    "                }\n",
    "            ],\n",
    "            'tabular_model_configuration': {\n",
    "                'enable_model_report': False\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa2f804",
   "metadata": {},
   "source": [
    "### 4.2 MOSTLY AI Training Process\n",
    "\n",
    "**Training Process:**\n",
    "1. **Upload Data**: Send training data securely to MOSTLY AI cloud\n",
    "2. **Model Configuration**: Apply the complex multi-table configuration\n",
    "3. **AI Training**: Use advanced generative models including LLMs\n",
    "4. **Quality Validation**: Automatic quality checks during training\n",
    "\n",
    "**TabularARGN**\n",
    "- Flexible and efficient auto-regressive framework for generating high-fidelity synthetic data\n",
    "- Arxiv paper: https://arxiv.org/abs/2501.12012\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0dc476",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Train MOSTLY AI generator with our configuration\n",
    "g = mostly.train(config=config, start=True, wait=True)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_minutes = (end_time - start_time) / 60\n",
    "\n",
    "print(f\"‚è±Ô∏è Total training time: {elapsed_minutes:.2f} minutes\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6dd068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "TARGET_FRACTION = 0.10\n",
    "target_size = max(1, int(len(org_train) * TARGET_FRACTION))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"üìä Generating {target_size:,} synthetic organization records \"\n",
    "      f\"(~{TARGET_FRACTION:.0%} of {len(org_train):,})...\")\n",
    "\n",
    "# Generate synthetic data\n",
    "sd = mostly.generate(g, size=target_size)\n",
    "mostlyai_synthetic_data = sd.data()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_minutes = (end_time - start_time) / 60\n",
    "\n",
    "# Calculate generation statistics\n",
    "total_records = sum(len(mostlyai_synthetic_data[table]) for table in mostlyai_synthetic_data.keys())\n",
    "generation_rate = total_records / (end_time - start_time)\n",
    "\n",
    "# Performance metrics\n",
    "print(f\"‚è±Ô∏è Generation time: {elapsed_minutes:.2f} minutes\")\n",
    "print(f\"üöÄ Generation rate: {generation_rate:,.0f} records/second\")\n",
    "\n",
    "print(\"üìä Synthetic data breakdown:\")\n",
    "for table_name in mostlyai_synthetic_data:\n",
    "    print(f\"   - {table_name.capitalize()}: {len(mostlyai_synthetic_data[table_name]):,} rows\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42981d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save MOSTLY AI synthetic data for comparison\n",
    "output_folder = './data/referential-integrity/'\n",
    "\n",
    "mostlyai_files = {\n",
    "    'organizations': f'{output_folder}mostlyai_organizations.parquet',\n",
    "    'relations': f'{output_folder}mostlyai_relations.parquet',\n",
    "}\n",
    "\n",
    "for table_name, file_path in mostlyai_files.items():\n",
    "    mostlyai_synthetic_data[table_name].to_parquet(file_path, index=False)\n",
    "    print(f\"üíæ Saved {table_name} synthetic data to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3a4442",
   "metadata": {},
   "source": [
    "## 5. Synthetic Data Quality Assessment\n",
    "\n",
    "After generating synthetic data using both SDV and MOSTLY AI, it's crucial to comprehensively evaluate the quality, privacy, and integrity of the generated datasets. This section provides a multi-faceted quality assessment framework that ensures our synthetic data meets production standards.\n",
    "\n",
    "### **Quality Assessment Framework:**\n",
    "\n",
    "Our evaluation methodology consists of two complementary approaches:\n",
    "\n",
    "1. **Statistical Quality Assessment (Section 5.1)**: Using the MOSTLY AI QA library to evaluate statistical fidelity, privacy metrics, and overall data quality for the relations table (the subject of this experiment)\n",
    "2. **Foreign Key Integrity Verification (Section 5.2)**: Custom verification to ensure referential integrity and relationship preservation in multi-table synthetic data\n",
    "\n",
    "### **Key Quality Dimensions Evaluated:**\n",
    "\n",
    "- **Statistical Accuracy**: How well synthetic data preserves statistical properties of the original data\n",
    "- **Privacy Protection**: Measurement of privacy risks and distance to closest record (DCR)\n",
    "- **Referential Integrity**: Verification that foreign key relationships are maintained correctly\n",
    "- **Utility vs Privacy Balance**: Evaluation of the trade-off between data utility and privacy protection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb74439d",
   "metadata": {},
   "source": [
    "## 5.1 Statistical Quality Assessment with MOSTLY AI QA Library\n",
    "\n",
    "The MOSTLY AI QA library provides enterprise-grade quality assessment capabilities that evaluate synthetic data across multiple dimensions. This assessment generates comprehensive HTML reports and quantitative metrics that help understand:\n",
    "\n",
    "- **Accuracy Scores**: Overall statistical fidelity of synthetic data\n",
    "- **Distance to Closest Record (DCR)**: Privacy risk measurement \n",
    "- **Univariate & Bivariate Distributions**: Preservation of individual column and column-pair statistics\n",
    "- **Correlation Analysis**: Maintenance of relationships between variables\n",
    "- **Similarity Metrics**: Overall resemblance to training data while avoiding overfitting\n",
    "\n",
    "These assessments are performed for the target dataset - relations, comparing synthetic data against both training and holdout datasets to ensure robust evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d663c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize the quality assessment framework\n",
    "from mostlyai import qa\n",
    "\n",
    "# Optionally run logging for a more detailed output\n",
    "# qa.init_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf9117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the split files from disk\n",
    "relations_train = pd.read_parquet('./data/relations_train.parquet')\n",
    "relations_test  = pd.read_parquet('./data/relations_test.parquet')\n",
    "\n",
    "print(\"‚úÖ Loaded splits:\")\n",
    "print(f\"  - relations_train: {relations_train.shape}\")\n",
    "print(f\"  - relations_test:  {relations_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ab36b2",
   "metadata": {},
   "source": [
    "## Evaluate Relations\n",
    "\n",
    "The relations table contains the `START_ID` and `END_ID` fields, which reference the organizations table. Our ultimate target output will show not only a statistically accurate organizations table, but one with entirely valid foreign key relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b208de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the split files from disk\n",
    "relations_train = pd.read_parquet('./data/relations_train.parquet')\n",
    "relations_test  = pd.read_parquet('./data/relations_test.parquet')\n",
    "\n",
    "print(\"‚úÖ Loaded splits:\")\n",
    "print(f\"  - org_train:       {org_train.shape}\")\n",
    "print(f\"  - org_test:        {org_test.shape}\")\n",
    "print(f\"  - relations_train: {relations_train.shape}\")\n",
    "print(f\"  - relations_test:  {relations_test.shape}\")\n",
    "\n",
    "# Load the SDV synthetic dataset\n",
    "sdv_relations = pd.read_parquet('./data/referential-integrity/sdv_relations.parquet')\n",
    "\n",
    "# Run QA with organizations as context table\n",
    "report_path, metrics = qa.report(\n",
    "    syn_tgt_data = sdv_relations,\n",
    "    trn_tgt_data = relations_train,\n",
    "    hol_tgt_data = relations_test,\n",
    "    syn_ctx_data = pd.read_parquet('./data/referential-integrity/sdv_organizations.parquet'),\n",
    "    trn_ctx_data = org_train,\n",
    "    hol_ctx_data = org_test,\n",
    "    ctx_primary_key = \"ID\",\n",
    "    tgt_context_key = \"START_ID\",   # modelled FK in SDV\n",
    "    max_sample_size_embeddings=10_000,\n",
    "    report_path='sdv_relations_qa_report.html'\n",
    ")\n",
    "\n",
    "print(f\"üìã SDV Relations Quality Report saved to: {report_path}\")\n",
    "print(\"\\nüìà SDV Relations Quality Metrics:\")\n",
    "print(metrics.model_dump_json(indent=4))\n",
    "\n",
    "# Print summary scores\n",
    "sdv_relations_accuracy = metrics.accuracy.overall\n",
    "sdv_relations_dcr_share = metrics.distances.dcr_share\n",
    "print(f\"\\nüéØ SDV Relations Summary:\")\n",
    "print(f\"   Overall Accuracy: {sdv_relations_accuracy:.3f}\")\n",
    "print(f\"   DCR Share: {sdv_relations_dcr_share:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc03cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_rel_path = './data/referential-integrity/mostlyai_relations.parquet'\n",
    "syn_org_path = './data/referential-integrity/mostlyai_organizations.parquet'\n",
    "syn_rel = pd.read_parquet(syn_rel_path).copy()\n",
    "syn_org = pd.read_parquet(syn_org_path).copy()\n",
    "\n",
    "# Cast dtypes to strings\n",
    "for df, kind in [(syn_rel, \"syn_rel\"), (relations_train, \"trn_rel\"), (relations_test, \"hol_rel\")]:\n",
    "    for col in (\"START_ID\", \"END_ID\", \"RELATION_ID\"):\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(\"string\")\n",
    "    for col in (\"TYPE\", \"STATUS\"):\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(\"string\")\n",
    "\n",
    "for df, kind in [(syn_org, \"syn_org\"), (org_train, \"trn_org\"), (org_test, \"hol_org\")]:\n",
    "    if \"ID\" in df.columns:\n",
    "        df[\"ID\"] = df[\"ID\"].astype(\"string\")\n",
    "    for col in (\"STATUS\", \"LEICATEGORY_NAME\", \"COUNTRY_NAME\"):\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(\"string\")\n",
    "\n",
    "# Diagnostics\n",
    "total_syn_rel = len(syn_rel)\n",
    "non_null_start = syn_rel[\"START_ID\"].notna().sum() if \"START_ID\" in syn_rel else 0\n",
    "matched_to_ctx = syn_rel[\"START_ID\"].isin(syn_org[\"ID\"]).sum() if \"START_ID\" in syn_rel and \"ID\" in syn_org else 0\n",
    "\n",
    "print(f\"üßÆ Synthetic relations: {total_syn_rel:,} rows \"\n",
    "      f\"(START_ID non-null: {non_null_start:,}, START_ID matching ctx.ID: {matched_to_ctx:,})\")\n",
    "\n",
    "MIN_ROWS = 100\n",
    "if matched_to_ctx < MIN_ROWS:\n",
    "    print(\"‚ö†Ô∏è Not enough usable synthetic rows for QA \"\n",
    "          f\"(need ‚â• {MIN_ROWS}, have {matched_to_ctx}).\")\n",
    "    print(\"üëâ Check that you‚Äôre loading the correct synthetic files and that keys match the context:\")\n",
    "    print(f\"   - syn_rel path: {syn_rel_path}\")\n",
    "    print(f\"   - syn_org path: {syn_org_path}\")\n",
    "    print(\"   - If you recently generated only a tiny sample, regenerate with a larger size for relations/organizations.\")\n",
    "else:\n",
    "    # Run QA\n",
    "    report_path, metrics = qa.report(\n",
    "        syn_tgt_data=syn_rel,\n",
    "        trn_tgt_data=relations_train,\n",
    "        hol_tgt_data=relations_test,\n",
    "        syn_ctx_data=syn_org,\n",
    "        trn_ctx_data=org_train,\n",
    "        hol_ctx_data=org_test,\n",
    "        ctx_primary_key=\"ID\",\n",
    "        tgt_context_key=\"START_ID\",   # one linkage at a time\n",
    "        max_sample_size_embeddings=1000,\n",
    "        report_path='mostlyai_relations_qa_report.html'\n",
    "    )\n",
    "    print(f\"üìã MOSTLY AI Relations Quality Report saved to: {report_path}\")\n",
    "\n",
    "    if metrics is None:\n",
    "        print(\"‚ö†Ô∏è QA returned no metrics (insufficient rows after internal filtering).\")\n",
    "    else:\n",
    "        print(\"\\nüìà MOSTLY AI Relations Quality Metrics:\")\n",
    "        print(metrics.model_dump_json(indent=4))\n",
    "        # Summary\n",
    "        print(\"\\nüéØ MOSTLY AI Relations Summary:\")\n",
    "        print(f\"   Overall Accuracy: {metrics.accuracy.overall:.3f}\")\n",
    "        print(f\"   DCR Share: {metrics.distances.dcr_share:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23af0974",
   "metadata": {},
   "source": [
    "## 6.2 Foreign Key Integrity Verification\n",
    "\n",
    "One critical aspect of multi-table synthetic data quality is ensuring that foreign key relationships are maintained properly. This section verifies that:\n",
    "\n",
    "1. **Referential Integrity**: All foreign keys in synthetic transfers reference valid customer IDs\n",
    "2. **Coverage**: All synthetic customers are properly referenced in the transfers table\n",
    "3. **Relationship Patterns**: The distribution of transfers per customer matches expected patterns\n",
    "\n",
    "This verification is essential for downstream applications that rely on proper table relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd845c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def verify_fk_integrity_orgs_relations(\n",
    "    org_df: pd.DataFrame,\n",
    "    rel_df: pd.DataFrame,\n",
    "    provider_name: str,\n",
    "    pk_col: str = \"ID\",\n",
    "    fk_cols: tuple = (\"START_ID\", \"END_ID\"),\n",
    "):\n",
    "    \"\"\"\n",
    "    Comprehensive foreign key integrity verification for GLEIF-style graph (two FKs).\n",
    "\n",
    "    Args:\n",
    "        org_df: Synthetic organizations dataframe (parent/nodes)\n",
    "        rel_df: Synthetic relations dataframe (child/edges)\n",
    "        provider_name: Name of the synthetic data provider (e.g., 'SDV', 'MOSTLY AI')\n",
    "        pk_col: Primary key column in org_df (default 'ID')\n",
    "        fk_cols: Tuple of foreign key columns in rel_df (default ('START_ID','END_ID'))\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with integrity and coverage metrics.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüè∑Ô∏è  {provider_name} Foreign Key Verification (relations ‚Üí organizations on {fk_cols})\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    missing_org = [c for c in [pk_col] if c not in org_df.columns]\n",
    "    missing_rel = [c for c in fk_cols if c not in rel_df.columns]\n",
    "    if missing_org or missing_rel:\n",
    "        raise KeyError(\n",
    "            f\"Missing required columns. organizations missing={missing_org}; relations missing={missing_rel}\"\n",
    "        )\n",
    "\n",
    "    org = org_df.copy()\n",
    "    rel = rel_df.copy()\n",
    "\n",
    "    # Align FK dtypes to PK dtype (fallback to string if needed)\n",
    "    pk_dtype = org[pk_col].dtype\n",
    "    for fk in fk_cols:\n",
    "        try:\n",
    "            rel[fk] = rel[fk].astype(pk_dtype)\n",
    "        except Exception:\n",
    "            # fallback: cast all to string\n",
    "            org[pk_col] = org[pk_col].astype(str)\n",
    "            rel[fk] = rel[fk].astype(str)\n",
    "            pk_dtype = org[pk_col].dtype  # update dtype\n",
    "\n",
    "    # Build sets for checks\n",
    "    org_ids = set(org[pk_col].dropna().unique())\n",
    "    fk_sets = {fk: set(rel[fk].dropna().unique()) for fk in fk_cols}\n",
    "\n",
    "    # Invalid FKs per column\n",
    "    invalid_per_fk = {fk: fk_sets[fk] - org_ids for fk in fk_cols}\n",
    "    total_invalid = sum(len(invalid_per_fk[fk]) for fk in fk_cols)\n",
    "\n",
    "    # Edge-level validity masks\n",
    "    valid_masks = {fk: rel[fk].isin(org_ids) for fk in fk_cols}\n",
    "    both_valid_mask = valid_masks[fk_cols[0]] & valid_masks[fk_cols[1]]\n",
    "    any_invalid_mask = ~(both_valid_mask)\n",
    "\n",
    "    both_valid_edges = int(both_valid_mask.sum())\n",
    "    total_edges = int(len(rel))\n",
    "    any_invalid_edges = int(any_invalid_mask.sum())\n",
    "    pct_both_valid = (both_valid_edges / total_edges * 100) if total_edges else 0.0\n",
    "    pct_any_invalid = (any_invalid_edges / total_edges * 100) if total_edges else 0.0\n",
    "\n",
    "    # Coverage: how many organizations are referenced by at least one endpoint\n",
    "    referenced_orgs = set(rel.loc[valid_masks[fk_cols[0]], fk_cols[0]].dropna()) | \\\n",
    "                      set(rel.loc[valid_masks[fk_cols[1]], fk_cols[1]].dropna())\n",
    "    coverage_count = len(referenced_orgs)\n",
    "    total_orgs = len(org_ids)\n",
    "    coverage_pct = (coverage_count / total_orgs * 100) if total_orgs else 0.0\n",
    "\n",
    "    # Degree distribution (count edges touching each org across BOTH endpoints)\n",
    "    # Build a single Series of all endpoints (valid or not), then count\n",
    "    endpoints = pd.concat([rel[fk_cols[0]], rel[fk_cols[1]]], ignore_index=True)\n",
    "    degree_counts = endpoints.value_counts(dropna=True)\n",
    "    avg_degree = float(degree_counts.mean()) if not degree_counts.empty else 0.0\n",
    "    median_degree = float(degree_counts.median()) if not degree_counts.empty else 0.0\n",
    "    max_degree = int(degree_counts.max()) if not degree_counts.empty else 0\n",
    "    min_degree = int(degree_counts.min()) if not degree_counts.empty else 0\n",
    "\n",
    "    referential_integrity_all = (total_invalid == 0) and (any_invalid_edges == 0)\n",
    "\n",
    "    # Results Summary\n",
    "    metrics = {\n",
    "        \"provider\": provider_name,\n",
    "        \"pk_col\": pk_col,\n",
    "        \"fk_cols\": list(fk_cols),\n",
    "        \"total_organizations\": total_orgs,\n",
    "        \"total_relations\": total_edges,\n",
    "        \"coverage_referenced_orgs\": coverage_count,\n",
    "        \"coverage_percentage\": coverage_pct,\n",
    "        \"invalid_fk_values_per_column\": {fk: len(invalid_per_fk[fk]) for fk in fk_cols},\n",
    "        \"total_edges_both_valid\": both_valid_edges,\n",
    "        \"total_edges_any_invalid\": any_invalid_edges,\n",
    "        \"pct_edges_both_valid\": pct_both_valid,\n",
    "        \"pct_edges_any_invalid\": pct_any_invalid,\n",
    "        \"referential_integrity_all\": referential_integrity_all,\n",
    "        \"degree_avg\": avg_degree,\n",
    "        \"degree_median\": median_degree,\n",
    "        \"degree_max\": max_degree,\n",
    "        \"degree_min\": min_degree,\n",
    "    }\n",
    "\n",
    "    # --- Print report ---\n",
    "    print(\"üìä Dataset Summary:\")\n",
    "    print(f\"   ‚Ä¢ Total Organizations: {total_orgs:,}\")\n",
    "    print(f\"   ‚Ä¢ Total Relations:     {total_edges:,}\")\n",
    "\n",
    "    print(\"\\nüîó Referential Integrity (per FK column):\")\n",
    "    for fk in fk_cols:\n",
    "        cnt = len(invalid_per_fk[fk])\n",
    "        status = \"‚úÖ PASSED\" if cnt == 0 else \"‚ùå FAILED\"\n",
    "        print(f\"   ‚Ä¢ {fk}: {status} ‚Äî {cnt:,} invalid value(s)\")\n",
    "\n",
    "    print(\"\\nüßÆ Edge-Level Integrity:\")\n",
    "    print(f\"   ‚Ä¢ Edges with BOTH endpoints valid: {both_valid_edges:,} ({pct_both_valid:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Edges with ANY invalid endpoint: {any_invalid_edges:,} ({pct_any_invalid:.1f}%)\")\n",
    "\n",
    "    print(\"\\nüìà Coverage (nodes referenced by ‚â•1 edge):\")\n",
    "    print(f\"   ‚Ä¢ Referenced Organizations: {coverage_count:,} ({coverage_pct:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Unreferenced Organizations: {max(total_orgs - coverage_count, 0):,}\")\n",
    "\n",
    "    print(\"\\nüìä Node Degree Distribution (edges per organization, counting both ends):\")\n",
    "    print(f\"   ‚Ä¢ Avg: {avg_degree:.2f}\")\n",
    "    print(f\"   ‚Ä¢ Median: {median_degree:.0f}\")\n",
    "    print(f\"   ‚Ä¢ Max: {max_degree:,}\")\n",
    "    print(f\"   ‚Ä¢ Min: {min_degree:,}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# SDV Synthetic Data\n",
    "sdv_orgs = pd.read_parquet('./data/referential-integrity/sdv_organizations.parquet')\n",
    "sdv_rels = pd.read_parquet('./data/referential-integrity/sdv_relations.parquet')\n",
    "\n",
    "# MOSTLY AI Synthetic Data\n",
    "mostlyai_orgs = pd.read_parquet('./data/referential-integrity/mostlyai_organizations.parquet')\n",
    "mostlyai_rels = pd.read_parquet('./data/referential-integrity/mostlyai_relations.parquet')\n",
    "\n",
    "# Run FK integrity checks\n",
    "sdv_metrics = verify_fk_integrity_orgs_relations(\n",
    "    org_df=sdv_orgs,\n",
    "    rel_df=sdv_rels,\n",
    "    provider_name=\"SDV\",\n",
    "    pk_col=\"ID\",\n",
    "    fk_cols=(\"START_ID\", \"END_ID\"),\n",
    ")\n",
    "\n",
    "mostlyai_metrics = verify_fk_integrity_orgs_relations(\n",
    "    org_df=mostlyai_orgs,\n",
    "    rel_df=mostlyai_rels,\n",
    "    provider_name=\"MOSTLY AI\",\n",
    "    pk_col=\"ID\",\n",
    "    fk_cols=(\"START_ID\", \"END_ID\"),\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
