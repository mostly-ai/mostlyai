{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Hugging Face Models for Air-Gapped Systems <a href=\"https://colab.research.google.com/github/mostly-ai/mostlyai/blob/main/docs/tutorials/model-download/model-download.ipynb\" target=\"_blank\"><img src=\"https://img.shields.io/badge/Open%20in-Colab-blue?logo=google-colab\" alt=\"Run on Colab\"></a>\n",
    "\n",
    "This notebook helps you download Hugging Face models for use in air-gapped systems. It will:\n",
    "1. Check which models are already in your cache\n",
    "2. Download any missing models\n",
    "3. Show you where the cached models are stored\n",
    "\n",
    "The downloaded models can then be copied to air-gapped systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models to download\n",
    "# Add or remove models as needed\n",
    "MODELS_TO_DOWNLOAD = [\n",
    "    # \"Qwen/Qwen2.5-1.5B\",           # Open model\n",
    "    # \"meta-llama/Llama-3.2-1B\",     # Gated model requiring HF_TOKEN\n",
    "    # \"microsoft/phi-1_5\",           # Open model\n",
    "    # \"Qwen/Qwen2.5-0.5B\",           # Open model\n",
    "    \"amd/AMD-Llama-135m\",  # Open model\n",
    "    \"HuggingFaceTB/SmolLM-135M\",  # Open model\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# using HF_TOKEN environment variable if set\n",
    "MY_HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)\n",
    "# or alternatively, you can set the token directly:\n",
    "# MY_HF_TOKEN = \"hf_...\"\n",
    "\n",
    "# using HF_HOME environment variable if set\n",
    "# MY_HF_HOME = os.environ.get(\"HF_HOME\", None)\n",
    "# or alternatively, you can set the home directory directly:\n",
    "# MY_HF_HOME = \"/path/to/your/cache\"\n",
    "# os.environ[\"HF_HOME\"] = MY_HF_HOME\n",
    "# or alternatively, you can set the home directory to the default cache directory:\n",
    "# MY_HF_HOME = Path.home() / \".cache\" / \"huggingface\"\n",
    "MY_HF_HOME = Path.home() / \"MY_HF_HOME_TEST\"\n",
    "os.environ[\"HF_HOME\"] = str(MY_HF_HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download, try_to_load_from_cache\n",
    "from typing import List, Optional\n",
    "import time\n",
    "\n",
    "\n",
    "def get_cache_dir() -> Path:\n",
    "    \"\"\"Get the cache directory.\"\"\"\n",
    "    cache_dir = Path(MY_HF_HOME) / \"hub\" if MY_HF_HOME else Path.home() / \".cache/huggingface/hub\"\n",
    "    Path.mkdir(cache_dir, parents=True, exist_ok=True)\n",
    "    return cache_dir\n",
    "\n",
    "\n",
    "def get_model_dir_name(model: str) -> Path:\n",
    "    \"\"\"Get the model directory.\"\"\"\n",
    "    return f\"models--{model.replace('/', '--')}\"\n",
    "\n",
    "\n",
    "def check_model_in_cache(model: str) -> bool:\n",
    "    \"\"\"Check if a model's config.json exists in cache.\"\"\"\n",
    "    try:\n",
    "        config_file = try_to_load_from_cache(\n",
    "            repo_id=model,\n",
    "            filename=\"config.json\",\n",
    "        )\n",
    "        return config_file is not None\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def download_language_model(model: str, token: Optional[str] = None, force_download: bool = False) -> None:\n",
    "    \"\"\"Download a language model if not already in cache.\n",
    "\n",
    "    Args:\n",
    "        model: The model identifier (e.g., 'Qwen/Qwen2.5-1.5B')\n",
    "        token: Optional Hugging Face token for gated models\n",
    "    \"\"\"\n",
    "    print(f\"\\nChecking model: {model}\")\n",
    "\n",
    "    if not force_download and check_model_in_cache(model):\n",
    "        print(f\"✓ Model {model} already in cache\")\n",
    "        return\n",
    "\n",
    "    print(f\"Downloading {model}...\")\n",
    "    try:\n",
    "        snapshot_download(\n",
    "            repo_id=model,\n",
    "            token=token,\n",
    "            # _use_symlinks=False\n",
    "        )\n",
    "        # sleep for 1 second to avoid rate limit\n",
    "        time.sleep(1)\n",
    "        print(f\"✓ {model} : Downloaded\")\n",
    "    except Exception as e:\n",
    "        if \"401\" in str(e):\n",
    "            print(f\"✗ {model} : Error: Model requires authentication. Please set a correct HF_TOKEN.\")\n",
    "        else:\n",
    "            print(f\"✗ {model} : Error during downloading: {str(e)}\")\n",
    "\n",
    "\n",
    "def list_available_models() -> List[str]:\n",
    "    \"\"\"List folders available in cache. Please note that this is not the same as the models available in the Hugging Face Hub.\"\"\"\n",
    "    cache_dir = get_cache_dir()\n",
    "    print(f\"Loading models from cache directory: {cache_dir}\")\n",
    "    if not cache_dir.exists():\n",
    "        return []\n",
    "\n",
    "    found_models = []\n",
    "    for m in MODELS_TO_DOWNLOAD:\n",
    "        config_file = try_to_load_from_cache(\n",
    "            repo_id=m,\n",
    "            filename=\"config.json\",\n",
    "        )\n",
    "        if config_file is None:\n",
    "            print(f\"✗ {m} : Error: `config.json` of model {m} not found in cache {cache_dir}\")\n",
    "        else:\n",
    "            # try to load the model with AutoModelForCausalLM\n",
    "            try:\n",
    "                from transformers import AutoModelForCausalLM\n",
    "\n",
    "                AutoModelForCausalLM.from_pretrained(\n",
    "                    m,\n",
    "                    use_cache=False,  # KV cache is not needed\n",
    "                    # trust_remote_code=False,  # avoid issues with remote code\n",
    "                    local_files_only=True,  # check integrity by only loading from cache\n",
    "                    token=MY_HF_TOKEN,  # needed for gated models\n",
    "                )\n",
    "                found_models.append(m)\n",
    "                print(f\"✓ {m} : Successfully loaded model\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ {m} : Error loading model: {str(e)}\")\n",
    "\n",
    "    # list only the subfolders of the cache directory, not recursively\n",
    "    subfolders = [f for f in cache_dir.glob(\"*\") if f.is_dir()]\n",
    "    print(\"-\" * 32)\n",
    "    print(f\"Cache directory: {cache_dir} has {len(subfolders)} folders.\")\n",
    "    print(\"Please note their names are not the same as the model names, as per the Hugging Face Hub naming convention\")\n",
    "    print(\"Found folders in cache are:\")\n",
    "    for f in subfolders:\n",
    "        print(f\"- {f}\")\n",
    "\n",
    "    return found_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double-check Available Models\n",
    "\n",
    "First, let's see which models are already in your cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking models in cache ...\")\n",
    "cached_models = list_available_models()\n",
    "print(\"-\" * 32)\n",
    "print(\"Models found currently in cache:\")\n",
    "if cached_models:\n",
    "    for model in cached_models:\n",
    "        print(f\"- {model}\")\n",
    "else:\n",
    "    print(\"No models found in cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hugging Face Token (Optional)\n",
    "\n",
    "Some models (like Llama) require authentication. If you need to download gated models, set your Hugging Face token here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and set your token if needed:\n",
    "# os.environ[\"HF_TOKEN\"] = \"your_token_here\"  # Get your token from: https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Models\n",
    "\n",
    "Now let's download any missing models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "for model in MODELS_TO_DOWNLOAD:\n",
    "    download_language_model(model, token, force_download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Cache Status\n",
    "\n",
    "Here are all the models now available in your cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final cache contents:\")\n",
    "cached_models = list_available_models()\n",
    "print(\"-\" * 32)\n",
    "print(\"Models found currently in cache:\")\n",
    "if cached_models:\n",
    "    for model in cached_models:\n",
    "        print(f\"- {model}\")\n",
    "else:\n",
    "    print(\"No models found in cache\")\n",
    "\n",
    "print(f\"\\nCache directory: {get_cache_dir()}\")\n",
    "print(f\"HF_HOME: {os.environ.get('HF_HOME')}\")\n",
    "print(\"\\nTo use these models on an air-gapped system:\")\n",
    "print(f\"1. Copy the entire contents of HF_HOME folder to a portable storage device: {MY_HF_HOME} \")\n",
    "print(\n",
    "    \"2. On the air-gapped system, create the directory path for HF_HOME and set the HF_HOME environment variable (or use default cache directory)\"\n",
    ")\n",
    "print(\"3. Copy the contents from your portable storage to the HF_HOME location on the air-gapped system\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
